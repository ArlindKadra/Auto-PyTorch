diff --git a/ReproduceBugWithHyperparameterSearchSpaceUpdates.ipynb b/ReproduceBugWithHyperparameterSearchSpaceUpdates.ipynb
new file mode 100644
index 0000000..6940576
--- /dev/null
+++ b/ReproduceBugWithHyperparameterSearchSpaceUpdates.ipynb
@@ -0,0 +1,413 @@
+{
+ "cells": [
+  {
+   "cell_type": "code",
+   "execution_count": 4,
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "from autoPyTorch import (AutoNetClassification, HyperparameterSearchSpaceUpdates)"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 5,
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "# Other imports for later usage\n",
+    "import pandas as pd\n",
+    "import numpy as np\n",
+    "import os as os\n",
+    "import openml\n",
+    "import json"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 6,
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "search_space_updates_cocktail = HyperparameterSearchSpaceUpdates()\n",
+    "search_space_updates_pn = HyperparameterSearchSpaceUpdates()\n",
+    "\n",
+    "# Fixed architecture space cocktail\n",
+    "search_space_updates_cocktail.append(\n",
+    "    node_name=\"NetworkSelector\",\n",
+    "    hyperparameter=\"shapedresnet:max_units\",\n",
+    "    value_range=[512],\n",
+    "    log=False,\n",
+    ")\n",
+    "search_space_updates_cocktail.append(\n",
+    "    node_name=\"NetworkSelector\",\n",
+    "    hyperparameter=\"shapedresnet:resnet_shape\",\n",
+    "    value_range=[\"brick\"],\n",
+    "    log=False,\n",
+    ")\n",
+    "search_space_updates_cocktail.append(\n",
+    "    node_name=\"NetworkSelector\",\n",
+    "    hyperparameter=\"shapedresnet:num_groups\",\n",
+    "    value_range=[4],\n",
+    "    log=False,\n",
+    ")\n",
+    "search_space_updates_cocktail.append(\n",
+    "    node_name=\"NetworkSelector\",\n",
+    "    hyperparameter=\"shapedresnet:blocks_per_group\",\n",
+    "    value_range=[2],\n",
+    "    log=False,\n",
+    ")\n",
+    "search_space_updates_cocktail.append(\n",
+    "    node_name=\"NetworkSelector\",\n",
+    "    hyperparameter=\"shapedresnet:use_shake_shake\",\n",
+    "    value_range=[False, True],\n",
+    "    log=False,\n",
+    ")\n",
+    "search_space_updates_cocktail.append(\n",
+    "    node_name=\"CreateDataLoader\",\n",
+    "    hyperparameter=\"batch_size\",\n",
+    "    value_range=[128],\n",
+    "    log=False,\n",
+    ")\n",
+    "search_space_updates_cocktail.append(\n",
+    "    node_name=\"NetworkSelector\",\n",
+    "    hyperparameter=\"shapedresnet:use_dropout\",\n",
+    "    value_range=[True, False],\n",
+    "    log=False,\n",
+    ")\n",
+    "search_space_updates_cocktail.append(\n",
+    "    node_name=\"NetworkSelector\",\n",
+    "    hyperparameter=\"shapedresnet:use_shake_shake\",\n",
+    "    value_range=[True, False],\n",
+    "    log=False,\n",
+    ")\n",
+    "search_space_updates_cocktail.append(\n",
+    "    node_name=\"NetworkSelector\",\n",
+    "    hyperparameter=\"shapedresnet:use_shake_drop\",\n",
+    "    value_range=[True, False],\n",
+    "    log=False,\n",
+    ")\n",
+    "search_space_updates_cocktail.append(\n",
+    "    node_name=\"NetworkSelector\",\n",
+    "    hyperparameter=\"shapedresnet:use_batch_normalization\",\n",
+    "    value_range=[True, False],\n",
+    "    log=False,\n",
+    ")\n",
+    "search_space_updates_cocktail.append(\n",
+    "    node_name=\"NetworkSelector\",\n",
+    "    hyperparameter=\"shapedresnet:use_skip_connection\",\n",
+    "    value_range=[True, False],\n",
+    "    log=False,\n",
+    ")"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 7,
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "search_space_updates_plain = HyperparameterSearchSpaceUpdates()\n",
+    "\n",
+    "# Fixed architecture space plain network\n",
+    "search_space_updates_plain.append(\n",
+    "    node_name=\"NetworkSelector\",\n",
+    "    hyperparameter=\"shapedresnet:max_units\",\n",
+    "    value_range=[512],\n",
+    "    log=False,\n",
+    ")\n",
+    "search_space_updates_plain.append(\n",
+    "    node_name=\"NetworkSelector\",\n",
+    "    hyperparameter=\"shapedresnet:resnet_shape\",\n",
+    "    value_range=[\"brick\"],\n",
+    "    log=False,\n",
+    ")\n",
+    "search_space_updates_plain.append(\n",
+    "    node_name=\"NetworkSelector\",\n",
+    "    hyperparameter=\"shapedresnet:num_groups\",\n",
+    "    value_range=[4],\n",
+    "    log=False,\n",
+    ")\n",
+    "search_space_updates_plain.append(\n",
+    "    node_name=\"NetworkSelector\",\n",
+    "    hyperparameter=\"shapedresnet:blocks_per_group\",\n",
+    "    value_range=[2],\n",
+    "    log=False,\n",
+    ")\n",
+    "search_space_updates_plain.append(\n",
+    "    node_name=\"CreateDataLoader\",\n",
+    "    hyperparameter=\"batch_size\",\n",
+    "    value_range=[128],\n",
+    "    log=False,\n",
+    ")\n",
+    "search_space_updates_plain.append(\n",
+    "    node_name=\"NetworkSelector\",\n",
+    "    hyperparameter=\"shapedresnet:use_dropout\",\n",
+    "    value_range=[False],\n",
+    "    log=False,\n",
+    ")\n",
+    "search_space_updates_plain.append(\n",
+    "    node_name=\"NetworkSelector\",\n",
+    "    hyperparameter=\"shapedresnet:use_shake_shake\",\n",
+    "    value_range=[False],\n",
+    "    log=False,\n",
+    ")\n",
+    "search_space_updates_plain.append(\n",
+    "    node_name=\"NetworkSelector\",\n",
+    "    hyperparameter=\"shapedresnet:use_shake_drop\",\n",
+    "    value_range=[False],\n",
+    "    log=False,\n",
+    ")\n",
+    "search_space_updates_plain.append(\n",
+    "    node_name=\"NetworkSelector\",\n",
+    "    hyperparameter=\"shapedresnet:use_batch_normalization\",\n",
+    "    value_range=[False],\n",
+    "    log=False,\n",
+    ")\n",
+    "search_space_updates_plain.append(\n",
+    "    node_name=\"NetworkSelector\",\n",
+    "    hyperparameter=\"shapedresnet:use_skip_connection\",\n",
+    "    value_range=[False],\n",
+    "    log=False,\n",
+    ")\n",
+    "search_space_updates_plain.append(\n",
+    "    node_name=\"OptimizerSelector\",\n",
+    "    hyperparameter=\"sgd:use_weight_decay\",\n",
+    "    value_range=[False],\n",
+    "    log=False,\n",
+    ")\n",
+    "search_space_updates_plain.append(\n",
+    "    node_name=\"OptimizerSelector\",\n",
+    "    hyperparameter=\"adamw:use_weight_decay\",\n",
+    "    value_range=[False],\n",
+    "    log=False,\n",
+    ")\n"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 8,
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "result_directory1 = '.\\\\logs_moon_cocktail'\n",
+    "#    'C:\\\\Users\\\\monum\\\\Desktop\\\\Project\\\\Auto-PyTorch\\\\regularization',\n",
+    "autonet_cocktail = AutoNetClassification(\n",
+    "    'C:\\\\Users\\\\monum\\\\Desktop\\\\Project\\\\Auto-PyTorch\\\\regularization',\n",
+    "    random_seed=1234,\n",
+    "    working_dir=result_directory1,\n",
+    "    hyperparameter_search_space_updates=search_space_updates_cocktail,\n",
+    "    result_logger_dir=result_directory1,\n",
+    ")"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 12,
+   "metadata": {},
+   "outputs": [
+    {
+     "name": "stdout",
+     "output_type": "stream",
+     "text": [
+      "Configuration space object:\n",
+      "  Hyperparameters:\n",
+      "    CreateDataLoader:batch_size, Type: Constant, Value: 128\n",
+      "    Imputation:strategy, Type: Categorical, Choices: {median}, Default: median\n",
+      "    InitializationSelector:initialization_method, Type: Categorical, Choices: {default}, Default: default\n",
+      "    InitializationSelector:initializer:initialize_bias, Type: Categorical, Choices: {Yes, No, Zero}, Default: Yes\n",
+      "    LearningrateSchedulerSelector:cosine_annealing:T_max, Type: UniformInteger, Range: [1, 20], Default: 10\n",
+      "    LearningrateSchedulerSelector:cosine_annealing:T_mult, Type: UniformFloat, Range: [1.0, 2.0], Default: 1.5\n",
+      "    LearningrateSchedulerSelector:exponential:gamma, Type: UniformFloat, Range: [0.8, 0.9999], Default: 0.89995\n",
+      "    LearningrateSchedulerSelector:lr_scheduler, Type: Categorical, Choices: {exponential, cosine_annealing, plateau}, Default: exponential\n",
+      "    LearningrateSchedulerSelector:plateau:factor, Type: UniformFloat, Range: [0.05, 0.5], Default: 0.275\n",
+      "    LearningrateSchedulerSelector:plateau:patience, Type: UniformInteger, Range: [3, 10], Default: 6\n",
+      "    LossModuleSelector:loss_module, Type: Categorical, Choices: {cross_entropy_weighted}, Default: cross_entropy_weighted\n",
+      "    NetworkSelector:lookahead:la_alpha, Type: UniformFloat, Range: [0.5, 0.8], Default: 0.632455532, on log-scale\n",
+      "    NetworkSelector:lookahead:la_steps, Type: UniformInteger, Range: [5, 10], Default: 8\n",
+      "    NetworkSelector:network, Type: Categorical, Choices: {shapedresnet}, Default: shapedresnet\n",
+      "    NetworkSelector:se_lastk, Type: UniformInteger, Range: [2, 20], Default: 6\n",
+      "    NetworkSelector:shapedresnet:activation, Type: Categorical, Choices: {sigmoid, tanh, relu}, Default: sigmoid\n",
+      "    NetworkSelector:shapedresnet:blocks_per_group, Type: Constant, Value: 2\n",
+      "    NetworkSelector:shapedresnet:max_units, Type: Constant, Value: 512\n",
+      "    NetworkSelector:shapedresnet:num_groups, Type: Constant, Value: 4\n",
+      "    NetworkSelector:shapedresnet:resnet_shape, Type: Constant, Value: brick\n",
+      "    NetworkSelector:shapedresnet:use_batch_normalization, Type: Constant, Value: 0\n",
+      "    NetworkSelector:shapedresnet:use_dropout, Type: Constant, Value: 0\n",
+      "    NetworkSelector:shapedresnet:use_shake_drop, Type: Constant, Value: 0\n",
+      "    NetworkSelector:shapedresnet:use_shake_shake, Type: Constant, Value: 0\n",
+      "    NetworkSelector:shapedresnet:use_skip_connection, Type: Constant, Value: 0\n",
+      "    NetworkSelector:use_lookahead, Type: Categorical, Choices: {True, False}, Default: False\n",
+      "    NetworkSelector:use_se, Type: Categorical, Choices: {True, False}, Default: False\n",
+      "    NetworkSelector:use_swa, Type: Categorical, Choices: {True, False}, Default: False\n",
+      "    NormalizationStrategySelector:normalization_strategy, Type: Categorical, Choices: {standardize}, Default: standardize\n",
+      "    OptimizerSelector:adamw:learning_rate, Type: UniformFloat, Range: [0.0001, 0.1], Default: 0.0031622777, on log-scale\n",
+      "    OptimizerSelector:adamw:use_weight_decay, Type: Constant, Value: 0\n",
+      "    OptimizerSelector:optimizer, Type: Categorical, Choices: {adamw, sgd}, Default: adamw\n",
+      "    OptimizerSelector:sgd:learning_rate, Type: UniformFloat, Range: [0.0001, 0.1], Default: 0.0031622777, on log-scale\n",
+      "    OptimizerSelector:sgd:momentum, Type: UniformFloat, Range: [0.1, 0.99], Default: 0.3146426545, on log-scale\n",
+      "    OptimizerSelector:sgd:use_weight_decay, Type: Constant, Value: 0\n",
+      "    PreprocessorSelector:preprocessor, Type: Categorical, Choices: {none}, Default: none\n",
+      "    ResamplingStrategySelector:over_sampling_method, Type: Categorical, Choices: {none}, Default: none\n",
+      "    ResamplingStrategySelector:target_size_strategy, Type: Categorical, Choices: {none, median, upsample}, Default: none\n",
+      "    ResamplingStrategySelector:under_sampling_method, Type: Categorical, Choices: {none, random}, Default: none\n",
+      "    TrainNode:batch_loss_computation_technique, Type: Categorical, Choices: {standard, mixup, cutout, cutmix}, Default: standard\n",
+      "    TrainNode:cutmix:beta, Type: Constant, Value: 1.0\n",
+      "    TrainNode:cutmix:cutmix_prob, Type: UniformFloat, Range: [0.0, 1.0], Default: 0.5\n",
+      "    TrainNode:cutout:cutout_prob, Type: UniformFloat, Range: [0.0, 1.0], Default: 0.5\n",
+      "    TrainNode:cutout:patch_ratio, Type: UniformFloat, Range: [0.0, 1.0], Default: 0.5\n",
+      "    TrainNode:mixup:alpha, Type: UniformFloat, Range: [0.0, 1.0], Default: 0.5\n",
+      "    TrainNode:use_adversarial_training, Type: Categorical, Choices: {True, False}, Default: True\n",
+      "  Conditions:\n",
+      "    LearningrateSchedulerSelector:cosine_annealing:T_max | LearningrateSchedulerSelector:lr_scheduler == 'cosine_annealing'\n",
+      "    LearningrateSchedulerSelector:cosine_annealing:T_mult | LearningrateSchedulerSelector:lr_scheduler == 'cosine_annealing'\n",
+      "    LearningrateSchedulerSelector:exponential:gamma | LearningrateSchedulerSelector:lr_scheduler == 'exponential'\n",
+      "    LearningrateSchedulerSelector:plateau:factor | LearningrateSchedulerSelector:lr_scheduler == 'plateau'\n",
+      "    LearningrateSchedulerSelector:plateau:patience | LearningrateSchedulerSelector:lr_scheduler == 'plateau'\n",
+      "    NetworkSelector:lookahead:la_alpha | NetworkSelector:use_lookahead == True\n",
+      "    NetworkSelector:lookahead:la_steps | NetworkSelector:use_lookahead == True\n",
+      "    NetworkSelector:se_lastk | NetworkSelector:use_se == True\n",
+      "    NetworkSelector:shapedresnet:activation | NetworkSelector:network == 'shapedresnet'\n",
+      "    NetworkSelector:shapedresnet:blocks_per_group | NetworkSelector:network == 'shapedresnet'\n",
+      "    NetworkSelector:shapedresnet:max_units | NetworkSelector:network == 'shapedresnet'\n",
+      "    NetworkSelector:shapedresnet:num_groups | NetworkSelector:network == 'shapedresnet'\n",
+      "    NetworkSelector:shapedresnet:resnet_shape | NetworkSelector:network == 'shapedresnet'\n",
+      "    NetworkSelector:shapedresnet:use_batch_normalization | NetworkSelector:network == 'shapedresnet'\n",
+      "    NetworkSelector:shapedresnet:use_dropout | NetworkSelector:network == 'shapedresnet'\n",
+      "    NetworkSelector:shapedresnet:use_shake_drop | NetworkSelector:network == 'shapedresnet'\n",
+      "    NetworkSelector:shapedresnet:use_shake_shake | NetworkSelector:network == 'shapedresnet'\n",
+      "    NetworkSelector:shapedresnet:use_skip_connection | NetworkSelector:network == 'shapedresnet'\n",
+      "    OptimizerSelector:adamw:learning_rate | OptimizerSelector:optimizer == 'adamw'\n",
+      "    OptimizerSelector:adamw:use_weight_decay | OptimizerSelector:optimizer == 'adamw'\n",
+      "    OptimizerSelector:sgd:learning_rate | OptimizerSelector:optimizer == 'sgd'\n",
+      "    OptimizerSelector:sgd:momentum | OptimizerSelector:optimizer == 'sgd'\n",
+      "    OptimizerSelector:sgd:use_weight_decay | OptimizerSelector:optimizer == 'sgd'\n",
+      "    TrainNode:cutmix:beta | TrainNode:batch_loss_computation_technique == 'cutmix'\n",
+      "    TrainNode:cutmix:cutmix_prob | TrainNode:batch_loss_computation_technique == 'cutmix'\n",
+      "    TrainNode:cutout:cutout_prob | TrainNode:batch_loss_computation_technique == 'cutout'\n",
+      "    TrainNode:cutout:patch_ratio | TrainNode:batch_loss_computation_technique == 'cutout'\n",
+      "    TrainNode:mixup:alpha | TrainNode:batch_loss_computation_technique == 'mixup'\n",
+      "    TrainNode:use_adversarial_training | TrainNode:batch_loss_computation_technique == 'standard'\n",
+      "  Forbidden Clauses:\n",
+      "    (Forbidden: NetworkSelector:use_swa == True && Forbidden: NetworkSelector:use_se == True)\n",
+      "\n"
+     ]
+    }
+   ],
+   "source": [
+    "# Get the current configuration as dict\n",
+    "current_configuration = autonet_cocktail.get_current_autonet_config()\n",
+    "\n",
+    "# Get the ConfigSpace object with all hyperparameters, conditions, default values and default ranges\n",
+    "hyperparameter_search_space = autonet_cocktail.get_hyperparameter_search_space()\n",
+    "\n",
+    "# Print all possible configuration options \n",
+    "#autonet.print_help()\n",
+    "\n",
+    "## Note that we in search_space_updates_cocktail\n",
+    "## use_dropout is set to [True, False]\n",
+    "## but in search_space_updates_plain use_dropout is set to [False]\n",
+    "## which overwrites when since we create autonet object after search_space_updates_plain update\n",
+    "\n",
+    "print(hyperparameter_search_space)"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "## Now we do update after creating autonet object\n",
+    "# Fixed architecture space cocktail\n",
+    "search_space_updates_cocktail.append(\n",
+    "    node_name=\"NetworkSelector\",\n",
+    "    hyperparameter=\"shapedresnet:max_units\",\n",
+    "    value_range=[512],\n",
+    "    log=False,\n",
+    ")\n",
+    "search_space_updates_cocktail.append(\n",
+    "    node_name=\"NetworkSelector\",\n",
+    "    hyperparameter=\"shapedresnet:resnet_shape\",\n",
+    "    value_range=[\"brick\"],\n",
+    "    log=False,\n",
+    ")\n",
+    "search_space_updates_cocktail.append(\n",
+    "    node_name=\"NetworkSelector\",\n",
+    "    hyperparameter=\"shapedresnet:num_groups\",\n",
+    "    value_range=[4],\n",
+    "    log=False,\n",
+    ")\n",
+    "search_space_updates_cocktail.append(\n",
+    "    node_name=\"NetworkSelector\",\n",
+    "    hyperparameter=\"shapedresnet:blocks_per_group\",\n",
+    "    value_range=[2],\n",
+    "    log=False,\n",
+    ")\n",
+    "search_space_updates_cocktail.append(\n",
+    "    node_name=\"NetworkSelector\",\n",
+    "    hyperparameter=\"shapedresnet:use_shake_shake\",\n",
+    "    value_range=[False, True],\n",
+    "    log=False,\n",
+    ")\n",
+    "search_space_updates_cocktail.append(\n",
+    "    node_name=\"CreateDataLoader\",\n",
+    "    hyperparameter=\"batch_size\",\n",
+    "    value_range=[128],\n",
+    "    log=False,\n",
+    ")\n",
+    "search_space_updates_cocktail.append(\n",
+    "    node_name=\"NetworkSelector\",\n",
+    "    hyperparameter=\"shapedresnet:use_dropout\",\n",
+    "    value_range=[True, False],\n",
+    "    log=False,\n",
+    ")\n",
+    "search_space_updates_cocktail.append(\n",
+    "    node_name=\"NetworkSelector\",\n",
+    "    hyperparameter=\"shapedresnet:use_shake_shake\",\n",
+    "    value_range=[True, False],\n",
+    "    log=False,\n",
+    ")\n",
+    "search_space_updates_cocktail.append(\n",
+    "    node_name=\"NetworkSelector\",\n",
+    "    hyperparameter=\"shapedresnet:use_shake_drop\",\n",
+    "    value_range=[True, False],\n",
+    "    log=False,\n",
+    ")\n",
+    "search_space_updates_cocktail.append(\n",
+    "    node_name=\"NetworkSelector\",\n",
+    "    hyperparameter=\"shapedresnet:use_batch_normalization\",\n",
+    "    value_range=[True, False],\n",
+    "    log=False,\n",
+    ")\n",
+    "search_space_updates_cocktail.append(\n",
+    "    node_name=\"NetworkSelector\",\n",
+    "    hyperparameter=\"shapedresnet:use_skip_connection\",\n",
+    "    value_range=[True, False],\n",
+    "    log=False,\n",
+    ")"
+   ]
+  }
+ ],
+ "metadata": {
+  "kernelspec": {
+   "display_name": "Python 3",
+   "language": "python",
+   "name": "python3"
+  },
+  "language_info": {
+   "codemirror_mode": {
+    "name": "ipython",
+    "version": 3
+   },
+   "file_extension": ".py",
+   "mimetype": "text/x-python",
+   "name": "python",
+   "nbconvert_exporter": "python",
+   "pygments_lexer": "ipython3",
+   "version": "3.7.4"
+  }
+ },
+ "nbformat": 4,
+ "nbformat_minor": 2
+}
diff --git a/autoPyTorch/components/networks/feature/shapedresnet.py b/autoPyTorch/components/networks/feature/shapedresnet.py
index 7fa1784..ed7cf8a 100644
--- a/autoPyTorch/components/networks/feature/shapedresnet.py
+++ b/autoPyTorch/components/networks/feature/shapedresnet.py
@@ -58,19 +58,41 @@ class ShapedResNet(ResNet):
         use_skip_connection=(True, False),
     ):
         cs = CS.ConfigurationSpace()
-        
+
         num_groups_hp = get_hyperparameter(CS.UniformIntegerHyperparameter, "num_groups", num_groups)
         cs.add_hyperparameter(num_groups_hp)
         blocks_per_group_hp = get_hyperparameter(CS.UniformIntegerHyperparameter, "blocks_per_group", blocks_per_group)
         cs.add_hyperparameter(blocks_per_group_hp)
         add_hyperparameter(cs, CS.CategoricalHyperparameter, "activation", activation)
         use_dropout_hp = add_hyperparameter(cs, CS.CategoricalHyperparameter, "use_dropout", use_dropout)
-        add_hyperparameter(cs, CS.CategoricalHyperparameter, "use_shake_shake", use_shake_shake)
+        # shake_shake_hp = cs.add_hyperparameter(CSH.CategoricalHyperparameter(name="use_shake_shake", choices=use_shake_shake, default_value=False))
+        shake_shake_hp = add_hyperparameter(cs, CS.CategoricalHyperparameter, "use_shake_shake", use_shake_shake)
         add_hyperparameter(cs, CS.CategoricalHyperparameter, "use_batch_normalization", use_batch_normalization)
-        add_hyperparameter(cs, CS.CategoricalHyperparameter, "use_skip_connection", use_skip_connection)
-
+        # skip_connection_hp = cs.add_hyperparameter(CSH.CategoricalHyperparameter(name="use_skip_connection", choices=use_skip_connection))
+        skip_connection_hp = add_hyperparameter(cs, CS.CategoricalHyperparameter, "use_skip_connection", use_skip_connection)
+        # shake_drop_hp = cs.add_hyperparameter(CSH.CategoricalHyperparameter(name="use_shake_drop", choices=use_shake_drop, default_value=False))
         shake_drop_hp = add_hyperparameter(cs, CS.CategoricalHyperparameter, "use_shake_drop", use_shake_drop)
 
+        ## Forbid a shit load of things :xD
+        if True in use_shake_shake:
+            forbid_shake_shake = CS.ForbiddenEqualsClause(shake_shake_hp, True)
+        if True in use_shake_drop:
+            forbid_shake_drop = CS.ForbiddenEqualsClause(shake_drop_hp, True)
+        if False in use_skip_connection:
+            when_no_skip_con = CS.ForbiddenEqualsClause(skip_connection_hp, False)
+
+        if True in use_shake_shake and False in use_skip_connection:
+            forbidden_clause2 = CS.ForbiddenAndConjunction(forbid_shake_shake, when_no_skip_con)
+            cs.add_forbidden_clause(forbidden_clause2)
+        
+        if True in use_shake_drop and False in use_skip_connection:
+            forbidden_clause3 = CS.ForbiddenAndConjunction(forbid_shake_drop, when_no_skip_con)
+            cs.add_forbidden_clause(forbidden_clause3)
+
+        if True in use_shake_shake and True in use_shake_drop:
+            forbidden_clause1 = CS.ForbiddenAndConjunction(forbid_shake_shake, forbid_shake_drop)
+            cs.add_forbidden_clause(forbidden_clause1)
+
         validate_if_activated = False
         if isinstance(use_shake_drop, tuple):
             if isinstance(use_shake_drop[0], list):
diff --git a/autoPyTorch/components/optimizer/optimizer.py b/autoPyTorch/components/optimizer/optimizer.py
index ae1ab20..e001bd4 100644
--- a/autoPyTorch/components/optimizer/optimizer.py
+++ b/autoPyTorch/components/optimizer/optimizer.py
@@ -346,7 +346,7 @@ class Lookahead(Optimizer):
     @staticmethod
     def get_config_space(
             la_steps=((5, 10), False),
-            la_alpha=((0.5, 0.8), False),
+            la_alpha=((0.5, 0.8), True),
     ):
         cs = CS.ConfigurationSpace()
         add_hyperparameter(cs, CS.UniformIntegerHyperparameter, 'la_steps', la_steps)
diff --git a/autoPyTorch/components/training/lr_scheduling.py b/autoPyTorch/components/training/lr_scheduling.py
index cbd25ce..ec21883 100644
--- a/autoPyTorch/components/training/lr_scheduling.py
+++ b/autoPyTorch/components/training/lr_scheduling.py
@@ -31,15 +31,9 @@ class LrScheduling(BaseTrainingTechnique):
             return
 
         if self.lr_step_with_time:
-            if isinstance(log['loss'], list):
-                log["lr_scheduler_converged"] = self.perform_scheduling(trainer, time.time() - trainer.fit_start_time, log['loss'][-1])
-            else:
-                log["lr_scheduler_converged"] = self.perform_scheduling(trainer, time.time() - trainer.fit_start_time, log['loss'])
+            log["lr_scheduler_converged"] = self.perform_scheduling(trainer, time.time() - trainer.fit_start_time, log['loss'])
         else:
-            if isinstance(log['loss'], list):
-                log["lr_scheduler_converged"]  = self.perform_scheduling(trainer, epoch, log['loss'][-1])
-            else:
-                log["lr_scheduler_converged"] = self.perform_scheduling(trainer, epoch, log['loss'])
+            log["lr_scheduler_converged"]  = self.perform_scheduling(trainer, epoch, log['loss'])
         return False
     
     def perform_scheduling(self, trainer, epoch, metric, **kwargs):
diff --git a/autoPyTorch/components/training/trainer.py b/autoPyTorch/components/training/trainer.py
index 5d0bb11..9fba275 100644
--- a/autoPyTorch/components/training/trainer.py
+++ b/autoPyTorch/components/training/trainer.py
@@ -100,10 +100,7 @@ class Trainer(object):
     def final_eval(self, opt_metric_name, logs, train_loader, valid_loader, best_over_epochs, refit):
         # select log
         if best_over_epochs:
-            if isinstance(logs[opt_metric_name], list):
-                final_log = min(logs, key=lambda log: self.metrics[0].loss_transform(log[opt_metric_name][-1]))
-            else:
-                final_log = min(logs, key=lambda log: self.metrics[0].loss_transform(log[opt_metric_name]))
+            final_log = min(logs, key=lambda log: self.metrics[0].loss_transform(log[opt_metric_name]))
         else:
             final_log = None
             for t in self.training_techniques:
@@ -123,18 +120,12 @@ class Trainer(object):
 
                 for i, metric in enumerate(self.metrics):
                     if valid_metric_results:
-                        if isinstance(final_log['val_' + metric.name], list):
-                            final_log['val_' + metric.name][-1] = valid_metric_results[i]
-                        else:
-                            final_log['val_' + metric.name] = valid_metric_results[i]
+                        final_log['val_' + metric.name] = valid_metric_results[i]
 
                 # TODO: What to do with the additional in case of snapshot ensembling
                 if self.eval_additional_logs_on_snapshot and not refit:
                         for additional_log in self.log_functions:
-                            if isinstance(final_log[additional_log.name], list):
-                                final_log[additional_log.name][-1] = additional_log(self.model, None)
-                            else:
-                                final_log[additional_log.name] = additional_log(self.model, None)
+                            final_log[additional_log.name] = additional_log(self.model, None)
 
             else:
                 self.model.load_snapshot()
@@ -144,16 +135,10 @@ class Trainer(object):
 
                 for i, metric in enumerate(self.metrics):
                     if valid_metric_results:
-                        if isinstance(final_log['val_' + metric.name], list):
-                            final_log['val_' + metric.name][-1] = valid_metric_results[i]
-                        else:
-                            final_log['val_' + metric.name] = valid_metric_results[i]
+                        final_log['val_' + metric.name] = valid_metric_results[i]
                 if self.eval_additional_logs_on_snapshot and not refit:
                         for additional_log in self.log_functions:
-                            if isinstance(final_log[additional_log.name], list):
-                                final_log[additional_log.name][-1] = additional_log(self.model, None)
-                            else:
-                                final_log[additional_log.name] = additional_log(self.model, None)
+                            final_log[additional_log.name] = additional_log(self.model, None)
         return final_log
 
     def train(self, epoch, train_loader):
diff --git a/autoPyTorch/core/worker.py b/autoPyTorch/core/worker.py
index 927fe07..b121282 100644
--- a/autoPyTorch/core/worker.py
+++ b/autoPyTorch/core/worker.py
@@ -112,7 +112,7 @@ class AutoNetWorker(Worker):
         """
         try:
             self.autonet_logger.info("Fit optimization pipeline")
-            return self.pipeline.fit_pipeline(hyperparameter_config=config, pipeline_config=self.pipeline_config,
+            return self.pipeline.fit_pipeline(hyperparameter_config=config, pipeline_config=self.pipeline_config, 
                                             X_train=self.X_train, Y_train=self.Y_train, X_valid=self.X_valid, Y_valid=self.Y_valid, 
                                             budget=budget, budget_type=self.budget_type, max_budget=self.max_budget, optimize_start_time=optimize_start_time,
                                             refit=False, rescore=False, hyperparameter_config_id=config_id, dataset_info=self.dataset_info)
diff --git a/autoPyTorch/pipeline/nodes/cross_validation.py b/autoPyTorch/pipeline/nodes/cross_validation.py
index 43cc9dd..242b9b0 100644
--- a/autoPyTorch/pipeline/nodes/cross_validation.py
+++ b/autoPyTorch/pipeline/nodes/cross_validation.py
@@ -118,12 +118,13 @@ class CrossValidation(SubPipelineNode):
 
         # aggregate logs
         logger.info("Aggregate the results across the splits")
+        df = pd.DataFrame(infos)
+        info = dict(df.mean())
         additional_results = self.process_additional_results(additional_results=additional_results, all_sub_pipeline_kwargs=all_sub_pipeline_kwargs,
             X=X, Y=Y, logger=logger)
-        #TODO save results accross folds
         loss = loss / num_cv_splits + loss_penalty
         logger.debug("Send additional results %s to master" % str(additional_results))
-        return dict({'loss': loss, 'info': infos}, **additional_results)
+        return dict({'loss': loss, 'info': info}, **additional_results)
 
     def predict(self, pipeline_config, X):
        
diff --git a/autoPyTorch/pipeline/nodes/metric_selector.py b/autoPyTorch/pipeline/nodes/metric_selector.py
index 9da5eb7..24ff72b 100644
--- a/autoPyTorch/pipeline/nodes/metric_selector.py
+++ b/autoPyTorch/pipeline/nodes/metric_selector.py
@@ -69,9 +69,6 @@ class MetricSelector(PipelineNode):
 
 
 def default_minimize_transform(value):
-
-    if isinstance(value, list):
-        value = value[-1]
     return 1 - value
 
 def no_transform(value):
diff --git a/autoPyTorch/pipeline/nodes/network_selector.py b/autoPyTorch/pipeline/nodes/network_selector.py
index 2116b45..58a968d 100644
--- a/autoPyTorch/pipeline/nodes/network_selector.py
+++ b/autoPyTorch/pipeline/nodes/network_selector.py
@@ -83,10 +83,10 @@ class NetworkSelector(PipelineNode):
 
         possible_networks = set(pipeline_config["networks"]).intersection(self.networks.keys())
         selector = cs.add_hyperparameter(CSH.CategoricalHyperparameter("network", possible_networks))
-        cs.add_hyperparameter(ConfigSpace.CategoricalHyperparameter("use_swa", pipeline_config["use_swa"]))
-        look_ahead = cs.add_hyperparameter(ConfigSpace.CategoricalHyperparameter("use_lookahead", pipeline_config["use_lookahead"]))
+        use_swa = cs.add_hyperparameter(ConfigSpace.CategoricalHyperparameter("use_swa", pipeline_config["use_swa"], default_value=False))
+        look_ahead = cs.add_hyperparameter(ConfigSpace.CategoricalHyperparameter("use_lookahead", pipeline_config["use_lookahead"], default_value=False))
         
-        use_se = cs.add_hyperparameter(ConfigSpace.CategoricalHyperparameter("use_se", pipeline_config["use_se"]))
+        use_se = cs.add_hyperparameter(ConfigSpace.CategoricalHyperparameter("use_se", pipeline_config["use_se"], default_value=False))
 
         if True in pipeline_config["use_se"]:
             se_lastk = ConfigSpace.UniformIntegerHyperparameter('se_lastk', 2, 20, default_value=6, log=False)
@@ -94,7 +94,6 @@ class NetworkSelector(PipelineNode):
             cond = ConfigSpace.EqualsCondition(se_lastk, use_se, True)
             cs.add_condition(cond)
             
-
         if True in pipeline_config["use_lookahead"]:
             cs.add_configuration_space(
                 prefix='lookahead',
@@ -103,6 +102,13 @@ class NetworkSelector(PipelineNode):
                 parent_hyperparameter={'parent': look_ahead, 'value': True}
             )
         
+        if (True in pipeline_config["use_se"]) and (True in pipeline_config["use_swa"]):
+            forbidden_clause = ConfigSpace.ForbiddenAndConjunction(
+                ConfigSpace.ForbiddenEqualsClause(use_swa, True),
+                ConfigSpace.ForbiddenEqualsClause(use_se, True)
+            )
+            cs.add_forbidden_clause(forbidden_clause)
+
         network_list = list()
         for network_name, network_type in self.networks.items():
             if (network_name not in possible_networks):
diff --git a/autoPyTorch/pipeline/nodes/optimization_algorithm.py b/autoPyTorch/pipeline/nodes/optimization_algorithm.py
index 8f591ac..132a626 100644
--- a/autoPyTorch/pipeline/nodes/optimization_algorithm.py
+++ b/autoPyTorch/pipeline/nodes/optimization_algorithm.py
@@ -309,7 +309,6 @@ class OptimizationAlgorithm(SubPipelineNode):
         except Exception as e:
             raise RuntimeError("Error parsing results. Check results.json and output for more details. An empty results.json is usually caused by a misconfiguration of AutoNet.")
 
-
         if (len(incumbent_trajectory['config_ids']) == 0):
             return dict()
         
diff --git a/autoPyTorch/pipeline/nodes/train_node.py b/autoPyTorch/pipeline/nodes/train_node.py
index b9b7027..032a911 100644
--- a/autoPyTorch/pipeline/nodes/train_node.py
+++ b/autoPyTorch/pipeline/nodes/train_node.py
@@ -153,10 +153,9 @@ class TrainNode(PipelineNode):
         logs = trainer.model.logs
         epoch = trainer.model.epochs_trained
         training_start_time = time.time()
-        log = dict()
-
         while True:
             # prepare epoch
+            log = dict()
             trainer.on_epoch_start(log=log, epoch=epoch)
             
             # training
@@ -170,29 +169,15 @@ class TrainNode(PipelineNode):
                     valid_metric_results = trainer.evaluate(valid_loader)
 
             # evaluate
-            if 'loss' in log:
-                log['loss'].append(train_loss)
-            else:
-                log['loss'] = [train_loss]
-
+            log['loss'] = train_loss
             for i, metric in enumerate(trainer.metrics):
-                if 'train_' + metric.name in log:
-                    log['train_' + metric.name].append(optimize_metric_results[i])
-                else:
-                    log['train_' + metric.name] = [optimize_metric_results[i]]
+                log['train_' + metric.name] = optimize_metric_results[i]
 
                 if valid_loader is not None and trainer.eval_valid_each_epoch:
-                    if 'val_' + metric.name in log:
-                        log['val_' + metric.name].append(valid_metric_results[i])
-                    else:
-                        log['val_' + metric.name] = [valid_metric_results[i]]
-
+                    log['val_' + metric.name] = valid_metric_results[i]
             if trainer.eval_additional_logs_each_epoch:
                 for additional_log in trainer.log_functions:
-                    if additional_log.name in log:
-                        log[additional_log.name].append(additional_log(trainer.model, epoch))
-                    else:
-                        log[additional_log.name] = [additional_log(trainer.model, epoch)]
+                    log[additional_log.name] = additional_log(trainer.model, epoch)
 
             # wrap up epoch
             stop_training = trainer.on_epoch_end(log=log, epoch=epoch) or stop_training
@@ -303,8 +288,8 @@ class TrainNode(PipelineNode):
             if len(self.ensemble_models) > 0:
                 Y = predict_se(self.ensemble_models, predict_loader)
             else:
-                print('Since snapshot ensembling is used, ensemble models are only stored if refit is called with final config')
-                raise('Cannot predict error!')
+                print('WARNING: snapshot ensembling is used, however, ensemble models contain only one model!')
+                Y = predict(network, predict_loader, device)
         else:
             Y = predict(network, predict_loader, device)
         return {'Y': Y.detach().cpu().numpy()}
diff --git a/benchmark_datasets.txt b/benchmark_datasets.txt
deleted file mode 100644
index 35bacea..0000000
--- a/benchmark_datasets.txt
+++ /dev/null
@@ -1 +0,0 @@
-233089 233090 233091 233092 233093 233088 233094 233095 233096 233097 233098 233099 233100 233101 233102 233103 233104 233105 233106 233107 233108 233109 233110 233111 233112 233113 233114 233115 233116 233117 233118 233119 233120 233121 233122 233123 233124 233125 233126 233127 233128 233129 233130 233131 233132 233133 233134 233135 233136 233137 233138 233139 233140 233141 233142 233143 233144 233145 233146 233147 233148
\ No newline at end of file
diff --git a/experiment_run.sh b/experiment_run.sh
deleted file mode 100755
index 33d726c..0000000
--- a/experiment_run.sh
+++ /dev/null
@@ -1,18 +0,0 @@
-#!/bin/bash
-
-file=$1
-export dir="$2"
-
-if ! [ -e "$file" ] ; then     # spaces inside square brackets
-    echo "$0: $file does not exist" >&2  # error message includes $0 and goes to stderr
-    exit 1                   # exit code is non-zero for error
-fi
-
-
-NUMBERS=$(<$file)
-for NUM in $NUMBERS
-do
-   export task=$(echo $NUM)     
-   msub -V -t 1-16 investigating_dl.moab
-done
-
diff --git a/no_regularization.txt b/no_regularization.txt
deleted file mode 100644
index 681789b..0000000
--- a/no_regularization.txt
+++ /dev/null
@@ -1,22 +0,0 @@
-optimize_metric=balanced_accuracy
-budget_type=time
-max_budget=3600
-min_budget=400
-memory_limit_mb=12000
-max_runtime=21600
-validation_split=0.2
-log_level=debug
-networks=['shapedresnet']
-embeddings=[none]
-lr_scheduler=[cosine_annealing,plateau,exponential]
-over_sampling_methods=[none]
-target_size_strategies=[none,upsample,median]
-preprocessors=[none]
-imputation_strategies=[median]
-initialization_methods=[default]
-loss_modules=[cross_entropy_weighted]
-normalization_strategies=[standardize]
-optimizer=[sgd, adamw]
-use_tensorboard_logger=False
-cuda=False
-use_pynisher=False
\ No newline at end of file
diff --git a/openml_experiment.py b/openml_experiment.py
index e2acb97..ca184be 100644
--- a/openml_experiment.py
+++ b/openml_experiment.py
@@ -1,16 +1,12 @@
 import argparse
 import json
 import os
-import re
 
 from autoPyTorch import (
     AutoNetClassification,
     HyperparameterSearchSpaceUpdates,
 )
 
-import autoPyTorch.pipeline.nodes as autonet_nodes
-from autoPyTorch.components.metrics.additional_logs import test_result
-
 import openml
 
 
@@ -155,12 +151,7 @@ parser.add_argument(
     default=64,
     type=int,
 )
-parser.add_argument(
-    '--num_threads',
-    help='Number of threads to use for the experiment.',
-    default=1,
-    type=int,
-)
+
 
 args = parser.parse_args()
 search_space_updates = HyperparameterSearchSpaceUpdates()
@@ -181,7 +172,7 @@ search_space_updates.append(
 search_space_updates.append(
     node_name="NetworkSelector",
     hyperparameter="shapedresnet:num_groups",
-    value_range=[2],
+    value_range=[4],
     log=False,
 )
 search_space_updates.append(
@@ -249,6 +240,7 @@ result_directory = os.path.join(
 
 os.makedirs(result_directory, exist_ok=True)
 
+
 task = openml.tasks.get_task(task_id=args.task_id)
 dataset = task.get_dataset()
 X, y, categorical_indicator, _ = dataset.get_data(
@@ -260,12 +252,10 @@ ind_train, ind_test = task.get_train_test_split_indices()
 X_train, Y_train = X[ind_train], y[ind_train]
 X_test, Y_test = X[ind_test], y[ind_test]
 
-run_id = re.sub(r"\D+\d+(\d|\])*$", "", args.run_id)
-
 autonet = AutoNetClassification(
    'no_regularization',
     random_seed=args.random_seed,
-    run_id=run_id,
+    run_id=args.run_id,
     task_id=args.array_id,
     categorical_features=categorical_indicator,
     min_workers=args.nr_workers,
@@ -278,14 +268,6 @@ autonet = AutoNetClassification(
     use_adversarial_training=[args.use_adversarial_training],
     hyperparameter_search_space_updates=search_space_updates,
     result_logger_dir=result_directory,
-    torch_num_threads=args.num_threads,
-    additional_logs=[test_result.__name__],
-)
-
-autonet.pipeline[autonet_nodes.LogFunctionsSelector.get_name()].add_log_function(
-    name= test_result.__name__,
-    log_function=test_result(autonet, X_test, Y_test),
-    loss_transform=False,
 )
 
 # Get the current configuration as dict
diff --git a/test_openml_exp.py b/test_openml_exp.py
new file mode 100644
index 0000000..981e296
--- /dev/null
+++ b/test_openml_exp.py
@@ -0,0 +1,259 @@
+import argparse
+import json
+import os
+import re
+
+from autoPyTorch import (
+    AutoNetClassification,
+    HyperparameterSearchSpaceUpdates,
+)
+
+import openml
+
+
+def str2bool(v):
+    if isinstance(v, bool):
+        return v
+    if v.lower() in ('yes', 'true', 't', 'y', '1'):
+        return True
+    elif v.lower() in ('no', 'false', 'f', 'n', '0'):
+        return False
+    else:
+        raise argparse.ArgumentTypeError('Boolean value expected.')
+
+
+parser = argparse.ArgumentParser(description='Configuration for the experiment')
+parser.add_argument(
+    '--run_id',
+    help='Unique id to identify the run.',
+    default='BOHB_Autonet',
+    type=str,
+)
+parser.add_argument(
+    '--array_id',
+    help='Array id to tread one job array as a HPB run.',
+    default=-1,
+    type=int,
+)
+parser.add_argument(
+    '--use_swa',
+    help='If stochastic weight averaging should be used.',
+    type=str2bool,
+    nargs='?',
+    const=True,
+    default=False,
+)
+parser.add_argument(
+    '--use_se',
+    help='If snapshot ensembling should be used.',
+    type=str2bool,
+    nargs='?',
+    const=True,
+    default=False,
+)
+parser.add_argument(
+    '--use_lookahead',
+    help='If the lookahead optimizing technique should be used.',
+    type=str2bool,
+    nargs='?',
+    const=True,
+    default=False,
+)
+parser.add_argument(
+    '--use_weight_decay',
+    help='If weight decay regularization should be used.',
+    type=str2bool,
+    nargs='?',
+    const=True,
+    default=False,
+)
+parser.add_argument(
+    '--use_batch_normalization',
+    help='If batch normalization regularization should be used.',
+    type=str2bool,
+    nargs='?',
+    const=True,
+    default=False,
+)
+parser.add_argument(
+    '--use_skip_connection',
+    help='If skip connections should be used. Turns the network into a residual network.',
+    type=str2bool,
+    nargs='?',
+    const=True,
+    default=False,
+)
+parser.add_argument(
+    '--use_dropout',
+    help='If dropout regularization should be used.',
+    type=str2bool,
+    nargs='?',
+    const=True,
+    default=False,
+)
+parser.add_argument(
+    '--use_shake_drop',
+    help='If shake drop regularization should be used.',
+    type=str2bool,
+    nargs='?',
+    const=True,
+    default=False,
+)
+parser.add_argument(
+    '--use_shake_shake',
+    help='If shake shake regularization should be used.',
+    type=str2bool,
+    nargs='?',
+    const=True,
+    default=False,
+)
+parser.add_argument(
+    '--use_adversarial_training',
+    help='If adversarial training should be used.',
+    type=str2bool,
+    nargs='?',
+    const=True,
+    default=False,
+)
+parser.add_argument(
+    '--example_augmentation',
+    help='If methods that augment examples should be used',
+    type=str,
+    choices=['mixup', 'cutout', 'cutmix', 'standard'],
+    default='standard',
+)
+parser.add_argument(
+    '--random_seed',
+    help='Random seed for the given experiment. It will be used for all workers',
+    default=11,
+    type=int,
+)
+parser.add_argument(
+    '--working_dir',
+    help='Working directory to store live data.',
+    default='.',
+    type=str,
+)
+parser.add_argument(
+    '--nr_workers',
+    help='Number of workers for the given experiment.',
+    default=1,
+    type=int,
+)
+parser.add_argument(
+    '--task_id',
+    help='Task id so that the dataset can be retrieved from OpenML.',
+    default=3,
+    type=int,
+)
+parser.add_argument(
+    '--nr_units',
+    help='Number of units per layer. To be used in the fixed architecture.',
+    default=64,
+    type=int,
+)
+
+
+args = parser.parse_args()
+search_space_updates = HyperparameterSearchSpaceUpdates()
+
+# Fixed architecture space
+search_space_updates.append(
+    node_name="NetworkSelector",
+    hyperparameter="shapedresnet:max_units",
+    value_range=[args.nr_units],
+    log=False,
+)
+search_space_updates.append(
+    node_name="NetworkSelector",
+    hyperparameter="shapedresnet:resnet_shape",
+    value_range=["brick"],
+    log=False,
+)
+search_space_updates.append(
+    node_name="NetworkSelector",
+    hyperparameter="shapedresnet:num_groups",
+    value_range=[2],
+    log=False,
+)
+search_space_updates.append(
+    node_name="NetworkSelector",
+    hyperparameter="shapedresnet:blocks_per_group",
+    value_range=[2],
+    log=False,
+)
+search_space_updates.append(
+    node_name="CreateDataLoader",
+    hyperparameter="batch_size",
+    value_range=[128],
+    log=False,
+)
+
+
+result_directory = os.path.join(
+    args.working_dir,
+    f'{args.nr_units}',
+    f'{args.task_id}',
+)
+
+os.makedirs(result_directory, exist_ok=True)
+
+task = openml.tasks.get_task(task_id=args.task_id)
+dataset = task.get_dataset()
+X, y, categorical_indicator, _ = dataset.get_data(
+    dataset_format='array',
+    target=dataset.default_target_attribute,
+)
+
+ind_train, ind_test = task.get_train_test_split_indices()
+X_train, Y_train = X[ind_train], y[ind_train]
+X_test, Y_test = X[ind_test], y[ind_test]
+
+run_id = re.sub(r"\D+\d+(\d|\])*$", "", args.run_id)
+
+autonet = AutoNetClassification(
+   '/home/fr/fr_fr/fr_rj50/Auto-PyTorch/no_regularization',
+    random_seed=args.random_seed,
+    run_id=run_id,
+    task_id=args.array_id,
+    categorical_features=categorical_indicator,
+    min_workers=args.nr_workers,
+    dataset_name=dataset.name,
+    working_dir=result_directory,
+    batch_loss_computation_techniques=[args.example_augmentation],
+    use_adversarial_training=[args.use_adversarial_training],
+    hyperparameter_search_space_updates=search_space_updates,
+    result_logger_dir=result_directory,
+)
+
+# Get the current configuration as dict
+current_configuration = autonet.get_current_autonet_config()
+
+# Get the ConfigSpace object with all hyperparameters, conditions, default values and default ranges
+hyperparameter_search_space = autonet.get_hyperparameter_search_space()
+print("Hyperparameter search space:")
+print(hyperparameter_search_space)
+
+# Print all possible configuration options
+autonet.print_help()
+
+results_fit = autonet.fit(
+    X_train=X_train,
+    Y_train=Y_train,
+    refit=True,
+)
+
+# Save fit results as json
+with open(os.path.join(result_directory, 'results_fit.json'), "w") as file:
+    json.dump(results_fit, file)
+
+# See how the random configuration performs (often it just predicts 0)
+score = autonet.score(X_test=X_test, Y_test=Y_test)
+pred = autonet.predict(X=X_test)
+
+print("Model prediction:", pred[0:10])
+print("Accuracy score", score)
+
+# Save fit results as json
+with open(os.path.join(result_directory, 'test_score.txt'), "w") as file:
+    json.dump(score, file)
